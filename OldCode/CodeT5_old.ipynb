{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CodeT5.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMak8KLRiQ07NOCdoZHHpqZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uxBTlcpL2iNf","executionInfo":{"status":"ok","timestamp":1651899045986,"user_tz":240,"elapsed":22068,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}},"outputId":"d445af9d-477d-4677-86b5-551d4666b34d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SS4eWkzwE2fg","executionInfo":{"status":"ok","timestamp":1651899055125,"user_tz":240,"elapsed":9252,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}},"outputId":"07abb2e4-e490-4370-c135-07f8b5f39657"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 13.2 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 60.6 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 65.1 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 75.9 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 6.0 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=f38eb046beef9f2b5c127f2f69f6b4ae503f983e1352cafed4136075088ff7a9\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sacremoses\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.53 tokenizers-0.12.1 transformers-4.18.0\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","source":["%cd /content/gdrive/Shareddrives/DL"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YKV-_rOc28Qn","executionInfo":{"status":"ok","timestamp":1651899129424,"user_tz":240,"elapsed":5,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}},"outputId":"177d3624-07d9-41a1-844a-3b275dfde085"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/Shareddrives/DL\n"]}]},{"cell_type":"markdown","source":[""],"metadata":{"id":"nRHXcSmH27My"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import TensorDataset\n","import numpy as np\n","import torch\n","from transformers import RobertaTokenizer, RobertaConfig, RobertaModel, RobertaForSequenceClassification, BertTokenizer\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from tqdm import tqdm\n","\n","import random\n","\n","seed_val = 17\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)"],"metadata":{"id":"7_M05DJYGolZ","executionInfo":{"status":"ok","timestamp":1651899153944,"user_tz":240,"elapsed":5492,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["tc2code = pd.read_json('./data.jsonl',lines=True)"],"metadata":{"id":"fRCo9D022cNP","executionInfo":{"status":"ok","timestamp":1651899161292,"user_tz":240,"elapsed":3992,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"r6PK1SVm3PAB","executionInfo":{"status":"ok","timestamp":1651899247190,"user_tz":240,"elapsed":223,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["print(tc2code.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bYHjwOgb3lQD","executionInfo":{"status":"ok","timestamp":1651899295287,"user_tz":240,"elapsed":5,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}},"outputId":"ca05dba3-46bf-49a9-da67-c37f6be3b219"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["                                            testCode  \\\n","0  /*\\n * This file was automatically generated b...   \n","1  /*\\n * This file was automatically generated b...   \n","2  /*\\n * This file was automatically generated b...   \n","3  /*\\n * This file was automatically generated b...   \n","4  /*\\n * This file was automatically generated b...   \n","\n","                                          sourceCode  \n","0  package macaw.util;\\n\\nimport macaw.system.Mac...  \n","1  package macaw.util;\\n\\nimport macaw.system.Use...  \n","2  package macaw.util;\\n\\nimport java.util.regex....  \n","3  package macaw.util;\\n\\nimport java.awt.event.C...  \n","4  package macaw.util;\\n\\nimport javax.swing.*;\\n...  \n"]}]},{"cell_type":"markdown","source":["Random data Explorations"],"metadata":{"id":"BtQP-oXd4B4I"}},{"cell_type":"code","source":["from transformers import T5Tokenizer, T5Model,T5ForConditionalGeneration\n","\n","tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-small')\n","model = T5ForConditionalGeneration.from_pretrained(\"Salesforce/codet5-small\").to(device)\n","# model=T5Model.from_pretrained(\"Salesforce/codet5-small\")\n","input_ids = tokenizer(\n","    tc2code['testCode'][0], return_tensors=\"pt\"\n",").input_ids  # Batch size 1\n","decoder_input_ids = tokenizer(tc2code['sourceCode'][0], return_tensors=\"pt\").input_ids  # Batch size 1\n","\n","# forward pass\n","# outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n","# last_hidden_states = outputs.last_hidden_state"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SqNltSsoL8dj","executionInfo":{"status":"ok","timestamp":1651899695753,"user_tz":240,"elapsed":3812,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}},"outputId":"65ad7f90-a962-4cd8-faae-dd00720f5310"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (1021 > 512). Running this sequence through the model will result in indexing errors\n"]}]},{"cell_type":"code","source":["sample_outputs = model.generate(\n","    input_ids.to(device),\n","    do_sample=True, \n","    max_length=50, \n","    top_k=50, \n","    top_p=0.95, \n","    num_return_sequences=3\n",")\n","\n","# outputs = model.generate(input_ids)\n","print(\"Generated docstring:\", tokenizer.decode(sample_outputs[0], skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q2Att9pqQiHC","executionInfo":{"status":"ok","timestamp":1651899846637,"user_tz":240,"elapsed":1604,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}},"outputId":"c6d07c40-dd73-4ebd-b968-fbf030eb2c2d"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated docstring:  public}if (label == \"Labellabelfalse ; if}\n","{\n","}\n","public voidthis {({\n","(;;{\n","\n"]}]},{"cell_type":"markdown","source":["Define Tokenizer and Tokenize the input and op"],"metadata":{"id":"mxsPokUj4lG9"}},{"cell_type":"code","source":["# tc2code['token_lengths']=tc2code['sourceCode'].apply(lambda x: len(tokenizer.tokenize(x)))"],"metadata":{"id":"Ee6-ogIg5FHF","executionInfo":{"status":"ok","timestamp":1651900060066,"user_tz":240,"elapsed":168008,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# tc2code['token_lengths_test']=tc2code['testCode'].apply(lambda x: len(tokenizer.tokenize(x)))"],"metadata":{"id":"MzuAstx96CGU","executionInfo":{"status":"ok","timestamp":1651900149933,"user_tz":240,"elapsed":89884,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# tc2code['token_lengths'].describe()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4P83bFSR7DN9","executionInfo":{"status":"ok","timestamp":1651900171961,"user_tz":240,"elapsed":235,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}},"outputId":"ed7825bb-e366-40c1-ed22-3e417410b266"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count     22947.000000\n","mean       1705.400488\n","std        3202.792489\n","min          13.000000\n","25%         419.500000\n","50%         832.000000\n","75%        1766.500000\n","max      112136.000000\n","Name: token_lengths, dtype: float64"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["The 75th percentile is 1766 hence we choose 1800 as the max_length"],"metadata":{"id":"jhDWVtCBC4Zt"}},{"cell_type":"code","source":["# tc2code['token_lengths_test'].describe()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TggHMadh7FVm","executionInfo":{"status":"ok","timestamp":1651900190270,"user_tz":240,"elapsed":270,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}},"outputId":"29e59294-ac3b-43f1-aa13-8ddc9355d1f8"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count     22947.000000\n","mean       1007.715039\n","std        3212.269785\n","min          96.000000\n","25%         243.000000\n","50%         406.000000\n","75%         972.000000\n","max      274693.000000\n","Name: token_lengths_test, dtype: float64"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["The 75th percentile is 972 so lets go with 1766"],"metadata":{"id":"SxyGEpgZC-BO"}},{"cell_type":"code","source":["def RobertaTokenizerForSeq2Seq(x):\n","  return tokenizer.batch_encode_plus(x,max_length=1800,truncation=True,padding='max_length',return_tensors=\"pt\", add_special_tokens = True)"],"metadata":{"id":"F1AaslSS495V","executionInfo":{"status":"ok","timestamp":1651900831992,"user_tz":240,"elapsed":271,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["def RobertaTokenizerForSeq2SeqTest(x):\n","  return tokenizer.batch_encode_plus(x,max_length=1000,truncation=True,padding='max_length',return_tensors=\"pt\", add_special_tokens = True)"],"metadata":{"id":"pDAT2UoiF497","executionInfo":{"status":"ok","timestamp":1651903032724,"user_tz":240,"elapsed":214,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["print(RobertaTokenizerForSeq2Seq(tc2code['sourceCode'][0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cxBuWHm-9G0a","executionInfo":{"status":"ok","timestamp":1651900718340,"user_tz":240,"elapsed":6,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}},"outputId":"0afd7f1a-94ea-4533-d876-897886df0a77"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[   1, 5610, 5318,  ...,    0,    0,    0]])\n"]}]},{"cell_type":"markdown","source":["Report ඞ"],"metadata":{"id":"hHmlJWEB7jtJ"}},{"cell_type":"code","source":["sample_code=tc2code.head(10)\n","sample_code['test_Samples']=sample_code['testCode'].apply(lambda x: tokenizer.tokenize(x))\n","sample_code['source_Samples']=sample_code['sourceCode'].apply(lambda x: tokenizer.tokenize(x))\n","sample_code.head(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":987},"id":"6Lvb5idf7QTu","executionInfo":{"status":"ok","timestamp":1651900787349,"user_tz":240,"elapsed":210,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}},"outputId":"4d038d70-c430-4848-bbbc-38661a2ca68e"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"]},{"output_type":"execute_result","data":{"text/plain":["                                            testCode  \\\n","0  /*\\n * This file was automatically generated b...   \n","1  /*\\n * This file was automatically generated b...   \n","2  /*\\n * This file was automatically generated b...   \n","3  /*\\n * This file was automatically generated b...   \n","4  /*\\n * This file was automatically generated b...   \n","5  /*\\n * This file was automatically generated b...   \n","6  /*\\n * This file was automatically generated b...   \n","7  /*\\n * This file was automatically generated b...   \n","8  /*\\n * This file was automatically generated b...   \n","9  /*\\n * This file was automatically generated b...   \n","\n","                                          sourceCode  token_lengths  \\\n","0  package macaw.util;\\n\\nimport macaw.system.Mac...           1271   \n","1  package macaw.util;\\n\\nimport macaw.system.Use...           2658   \n","2  package macaw.util;\\n\\nimport java.util.regex....            854   \n","3  package macaw.util;\\n\\nimport java.awt.event.C...            706   \n","4  package macaw.util;\\n\\nimport javax.swing.*;\\n...           1453   \n","5  package macaw.util;\\n\\nimport macaw.businessLa...           4626   \n","6  package macaw.util;\\n\\nimport macaw.system.Mac...           1150   \n","7  /*\\n * Created on 25-Jul-2009\\n * Copyright (C...           3194   \n","8  package macaw.util;\\n\\nimport macaw.system.Mac...           1150   \n","9  /*\\n * Created on 25-Jul-2009\\n * Copyright (C...           3194   \n","\n","   token_lengths_test                                       test_Samples  \\\n","0                1019  [/*, Ċ, Ġ*, ĠThis, Ġfile, Ġwas, Ġautomatically...   \n","1                2829  [/*, Ċ, Ġ*, ĠThis, Ġfile, Ġwas, Ġautomatically...   \n","2                 441  [/*, Ċ, Ġ*, ĠThis, Ġfile, Ġwas, Ġautomatically...   \n","3                 775  [/*, Ċ, Ġ*, ĠThis, Ġfile, Ġwas, Ġautomatically...   \n","4                 312  [/*, Ċ, Ġ*, ĠThis, Ġfile, Ġwas, Ġautomatically...   \n","5                 402  [/*, Ċ, Ġ*, ĠThis, Ġfile, Ġwas, Ġautomatically...   \n","6                 872  [/*, Ċ, Ġ*, ĠThis, Ġfile, Ġwas, Ġautomatically...   \n","7                 872  [/*, Ċ, Ġ*, ĠThis, Ġfile, Ġwas, Ġautomatically...   \n","8                2265  [/*, Ċ, Ġ*, ĠThis, Ġfile, Ġwas, Ġautomatically...   \n","9                2265  [/*, Ċ, Ġ*, ĠThis, Ġfile, Ġwas, Ġautomatically...   \n","\n","                                      source_Samples  \n","0  [package, Ġmac, aw, ., util, ;, Ċ, Ċ, import, ...  \n","1  [package, Ġmac, aw, ., util, ;, Ċ, Ċ, import, ...  \n","2  [package, Ġmac, aw, ., util, ;, Ċ, Ċ, import, ...  \n","3  [package, Ġmac, aw, ., util, ;, Ċ, Ċ, import, ...  \n","4  [package, Ġmac, aw, ., util, ;, Ċ, Ċ, import, ...  \n","5  [package, Ġmac, aw, ., util, ;, Ċ, Ċ, import, ...  \n","6  [package, Ġmac, aw, ., util, ;, Ċ, Ċ, import, ...  \n","7  [/*, Ċ, Ġ*, ĠCreated, Ġon, Ġ25, -, J, ul, -, 2...  \n","8  [package, Ġmac, aw, ., util, ;, Ċ, Ċ, import, ...  \n","9  [/*, Ċ, Ġ*, ĠCreated, Ġon, Ġ25, -, J, ul, -, 2...  "],"text/html":["\n","  <div id=\"df-86bce882-2161-4c47-a9c6-7dd90b701d2b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>testCode</th>\n","      <th>sourceCode</th>\n","      <th>token_lengths</th>\n","      <th>token_lengths_test</th>\n","      <th>test_Samples</th>\n","      <th>source_Samples</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/*\\n * This file was automatically generated b...</td>\n","      <td>package macaw.util;\\n\\nimport macaw.system.Mac...</td>\n","      <td>1271</td>\n","      <td>1019</td>\n","      <td>[/*, Ċ, Ġ*, ĠThis, Ġfile, Ġwas, Ġautomatically...</td>\n","      <td>[package, Ġmac, aw, ., util, ;, Ċ, Ċ, import, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/*\\n * This file was automatically generated b...</td>\n","      <td>package macaw.util;\\n\\nimport macaw.system.Use...</td>\n","      <td>2658</td>\n","      <td>2829</td>\n","      <td>[/*, Ċ, Ġ*, ĠThis, Ġfile, Ġwas, Ġautomatically...</td>\n","      <td>[package, Ġmac, aw, ., util, ;, Ċ, Ċ, import, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/*\\n * This file was automatically generated b...</td>\n","      <td>package macaw.util;\\n\\nimport java.util.regex....</td>\n","      <td>854</td>\n","      <td>441</td>\n","      <td>[/*, Ċ, Ġ*, ĠThis, Ġfile, Ġwas, Ġautomatically...</td>\n","      <td>[package, Ġmac, aw, ., util, ;, Ċ, Ċ, import, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/*\\n * This file was automatically generated b...</td>\n","      <td>package macaw.util;\\n\\nimport java.awt.event.C...</td>\n","      <td>706</td>\n","      <td>775</td>\n","      <td>[/*, Ċ, Ġ*, ĠThis, Ġfile, Ġwas, Ġautomatically...</td>\n","      <td>[package, Ġmac, aw, ., util, ;, Ċ, Ċ, import, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/*\\n * This file was automatically generated b...</td>\n","      <td>package macaw.util;\\n\\nimport javax.swing.*;\\n...</td>\n","      <td>1453</td>\n","      <td>312</td>\n","      <td>[/*, Ċ, Ġ*, ĠThis, Ġfile, Ġwas, Ġautomatically...</td>\n","      <td>[package, Ġmac, aw, ., util, ;, Ċ, Ċ, import, ...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>/*\\n * This file was automatically generated b...</td>\n","      <td>package macaw.util;\\n\\nimport macaw.businessLa...</td>\n","      <td>4626</td>\n","      <td>402</td>\n","      <td>[/*, Ċ, Ġ*, ĠThis, Ġfile, Ġwas, Ġautomatically...</td>\n","      <td>[package, Ġmac, aw, ., util, ;, Ċ, Ċ, import, ...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>/*\\n * This file was automatically generated b...</td>\n","      <td>package macaw.util;\\n\\nimport macaw.system.Mac...</td>\n","      <td>1150</td>\n","      <td>872</td>\n","      <td>[/*, Ċ, Ġ*, ĠThis, Ġfile, Ġwas, Ġautomatically...</td>\n","      <td>[package, Ġmac, aw, ., util, ;, Ċ, Ċ, import, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>/*\\n * This file was automatically generated b...</td>\n","      <td>/*\\n * Created on 25-Jul-2009\\n * Copyright (C...</td>\n","      <td>3194</td>\n","      <td>872</td>\n","      <td>[/*, Ċ, Ġ*, ĠThis, Ġfile, Ġwas, Ġautomatically...</td>\n","      <td>[/*, Ċ, Ġ*, ĠCreated, Ġon, Ġ25, -, J, ul, -, 2...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>/*\\n * This file was automatically generated b...</td>\n","      <td>package macaw.util;\\n\\nimport macaw.system.Mac...</td>\n","      <td>1150</td>\n","      <td>2265</td>\n","      <td>[/*, Ċ, Ġ*, ĠThis, Ġfile, Ġwas, Ġautomatically...</td>\n","      <td>[package, Ġmac, aw, ., util, ;, Ċ, Ċ, import, ...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>/*\\n * This file was automatically generated b...</td>\n","      <td>/*\\n * Created on 25-Jul-2009\\n * Copyright (C...</td>\n","      <td>3194</td>\n","      <td>2265</td>\n","      <td>[/*, Ċ, Ġ*, ĠThis, Ġfile, Ġwas, Ġautomatically...</td>\n","      <td>[/*, Ċ, Ġ*, ĠCreated, Ġon, Ġ25, -, J, ul, -, 2...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86bce882-2161-4c47-a9c6-7dd90b701d2b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-86bce882-2161-4c47-a9c6-7dd90b701d2b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-86bce882-2161-4c47-a9c6-7dd90b701d2b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["RobertaTokenizerForSeq2SeqTest(sample_code['testCode'].to_list())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SuBrtOXU9moe","executionInfo":{"status":"ok","timestamp":1651903054154,"user_tz":240,"elapsed":215,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}},"outputId":"c2aaee2d-5e23-4902-b59a-b6c5310e3fed"},"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[    1, 20308,   203,  ...,    67,  1919,     2],\n","        [    1, 20308,   203,  ...,   225,  1071,     2],\n","        [    1, 20308,   203,  ...,     0,     0,     0],\n","        ...,\n","        [    1, 20308,   203,  ...,     0,     0,     0],\n","        [    1, 20308,   203,  ...,   225,  1071,     2],\n","        [    1, 20308,   203,  ...,   225,  1071,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1]])}"]},"metadata":{},"execution_count":62}]},{"cell_type":"markdown","source":["Training the models hopefully"],"metadata":{"id":"DxedDAVk_NJX"}},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(tc2code['testCode'], tc2code['sourceCode'], test_size=0.2, shuffle=False)\n","X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.1, shuffle=False)"],"metadata":{"id":"YF7ZaNvZ7veJ","executionInfo":{"status":"ok","timestamp":1651903093862,"user_tz":240,"elapsed":2,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["X_train_tensors=RobertaTokenizerForSeq2SeqTest(X_train.to_list())\n","X_test_tensors=RobertaTokenizerForSeq2SeqTest(X_test.to_list())\n","X_val_tensors=RobertaTokenizerForSeq2SeqTest(X_val.to_list())"],"metadata":{"id":"8yTfw9ET_s6n","executionInfo":{"status":"ok","timestamp":1651903210808,"user_tz":240,"elapsed":111917,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["y_train_tensors=RobertaTokenizerForSeq2Seq(y_train.to_list())\n","y_test_tensors=RobertaTokenizerForSeq2Seq(y_test.to_list())\n","y_val_tensors=RobertaTokenizerForSeq2Seq(y_val.to_list())"],"metadata":{"id":"DsGd1lbCAKlb","executionInfo":{"status":"ok","timestamp":1651903397655,"user_tz":240,"elapsed":186862,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["input_ids_train = X_train_tensors['input_ids']\n","attention_masks_train = X_train_tensors['attention_mask']\n","labels_train = y_train_tensors['input_ids']\n","\n","#validation set\n","input_ids_val = X_val_tensors['input_ids']\n","attention_masks_val = X_val_tensors['attention_mask']\n","labels_val = y_val_tensors['input_ids']\n","\n","\n","input_ids_test = X_test_tensors['input_ids']\n","attention_masks_test = X_test_tensors['attention_mask']\n","labels_test = y_test_tensors['input_ids']\n","expected_source_code=y_test.to_list()"],"metadata":{"id":"a-2gh9C8BVJH","executionInfo":{"status":"ok","timestamp":1651904818898,"user_tz":240,"elapsed":216,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}}},"execution_count":95,"outputs":[]},{"cell_type":"code","source":["expected_source_code[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"id":"6feqxw73M0mX","executionInfo":{"status":"ok","timestamp":1651904828860,"user_tz":240,"elapsed":207,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}},"outputId":"bcea7a7c-8fd0-4fbb-bdb6-654bc0ab6d57"},"execution_count":96,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/*\\n * Created on 14/mar/2010\\n * Copyright (C) 2010 by Andrea Vacondio.\\n *\\n * This program is free software; you can redistribute it and/or modify it under the terms of the \\n * GNU General Public License as published by the Free Software Foundation; \\n * either version 2 of the License.\\n * This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; \\n * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. \\n * See the GNU General Public License for more details.\\n * You should have received a copy of the GNU General Public License along with this program; \\n * if not, write to the Free Software Foundation, Inc., \\n *  59 Temple Place, Suite 330, Boston, MA 02111-1307 USA\\n */\\npackage org.pdfsam.guiclient.business.listeners;\\n\\nimport java.awt.event.InputEvent;\\nimport java.awt.event.MouseWheelEvent;\\nimport java.awt.event.MouseWheelListener;\\n\\nimport org.pdfsam.guiclient.gui.components.JPreviewImage;\\n\\n/**\\n * Listen for the mouse wheel to perform zoom operations on the\\n * {@link JPreviewImage}\\n * \\n * @author Andrea Vacondio\\n * \\n */\\npublic class MouseWheelZoomListener implements MouseWheelListener {\\n\\n\\tprivate JPreviewImage image;\\n\\n\\t/**\\n\\t * @param image\\n\\t */\\n\\tpublic MouseWheelZoomListener(JPreviewImage image) {\\n\\t\\tsuper();\\n\\t\\tthis.image = image;\\n\\t}\\n\\n\\t/*\\n\\t * (non-Javadoc)\\n\\t * \\n\\t * @seejava.awt.event.MouseWheelListener#mouseWheelMoved(java.awt.event.\\n\\t * MouseWheelEvent)\\n\\t */\\n\\t@Override\\n\\tpublic void mouseWheelMoved(MouseWheelEvent e) {\\n\\t\\tif ((e.getModifiers() & InputEvent.CTRL_MASK) == InputEvent.CTRL_MASK) {\\n\\t\\t\\tif (e.getScrollType() == MouseWheelEvent.WHEEL_UNIT_SCROLL) {\\n\\t\\t\\t\\timage.zoom(e.getWheelRotation());\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n}\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":96}]},{"cell_type":"code","source":["import gc\n","gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"np4BZ3MzDPst","executionInfo":{"status":"ok","timestamp":1651903398002,"user_tz":240,"elapsed":359,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}},"outputId":"e0e70866-ebf3-4464-ce12-5f22560c207d"},"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"text/plain":["103"]},"metadata":{},"execution_count":67}]},{"cell_type":"markdown","source":["Training the model"],"metadata":{"id":"2XKSxIFZDSOk"}},{"cell_type":"code","source":["t5_model = T5ForConditionalGeneration.from_pretrained(\"Salesforce/codet5-small\",dropout_rate=0.5,d_model=1850,ignore_mismatched_sizes=True).to(device)\n","t5_model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qHW995YeDUnL","executionInfo":{"status":"ok","timestamp":1651903649745,"user_tz":240,"elapsed":3592,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}},"outputId":"7015ddc4-0a5b-407e-abb0-84c356a655d8"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at Salesforce/codet5-small and are newly initialized because the shapes did not match:\n","- shared.weight: found shape torch.Size([32100, 512]) in the checkpoint and torch.Size([32100, 1850]) in the model instantiated\n","- encoder.embed_tokens.weight: found shape torch.Size([32100, 512]) in the checkpoint and torch.Size([32100, 1850]) in the model instantiated\n","- encoder.block.0.layer.0.SelfAttention.q.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- encoder.block.0.layer.0.SelfAttention.k.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- encoder.block.0.layer.0.SelfAttention.v.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- encoder.block.0.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([1850, 512]) in the model instantiated\n","- encoder.block.0.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- encoder.block.0.layer.1.DenseReluDense.wi.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([2048, 1850]) in the model instantiated\n","- encoder.block.0.layer.1.DenseReluDense.wo.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([1850, 2048]) in the model instantiated\n","- encoder.block.0.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- encoder.block.1.layer.0.SelfAttention.q.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- encoder.block.1.layer.0.SelfAttention.k.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- encoder.block.1.layer.0.SelfAttention.v.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- encoder.block.1.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([1850, 512]) in the model instantiated\n","- encoder.block.1.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- encoder.block.1.layer.1.DenseReluDense.wi.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([2048, 1850]) in the model instantiated\n","- encoder.block.1.layer.1.DenseReluDense.wo.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([1850, 2048]) in the model instantiated\n","- encoder.block.1.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- encoder.block.2.layer.0.SelfAttention.q.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- encoder.block.2.layer.0.SelfAttention.k.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- encoder.block.2.layer.0.SelfAttention.v.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- encoder.block.2.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([1850, 512]) in the model instantiated\n","- encoder.block.2.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- encoder.block.2.layer.1.DenseReluDense.wi.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([2048, 1850]) in the model instantiated\n","- encoder.block.2.layer.1.DenseReluDense.wo.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([1850, 2048]) in the model instantiated\n","- encoder.block.2.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- encoder.block.3.layer.0.SelfAttention.q.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- encoder.block.3.layer.0.SelfAttention.k.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- encoder.block.3.layer.0.SelfAttention.v.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- encoder.block.3.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([1850, 512]) in the model instantiated\n","- encoder.block.3.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- encoder.block.3.layer.1.DenseReluDense.wi.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([2048, 1850]) in the model instantiated\n","- encoder.block.3.layer.1.DenseReluDense.wo.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([1850, 2048]) in the model instantiated\n","- encoder.block.3.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- encoder.block.4.layer.0.SelfAttention.q.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- encoder.block.4.layer.0.SelfAttention.k.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- encoder.block.4.layer.0.SelfAttention.v.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- encoder.block.4.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([1850, 512]) in the model instantiated\n","- encoder.block.4.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- encoder.block.4.layer.1.DenseReluDense.wi.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([2048, 1850]) in the model instantiated\n","- encoder.block.4.layer.1.DenseReluDense.wo.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([1850, 2048]) in the model instantiated\n","- encoder.block.4.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- encoder.block.5.layer.0.SelfAttention.q.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- encoder.block.5.layer.0.SelfAttention.k.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- encoder.block.5.layer.0.SelfAttention.v.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- encoder.block.5.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([1850, 512]) in the model instantiated\n","- encoder.block.5.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- encoder.block.5.layer.1.DenseReluDense.wi.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([2048, 1850]) in the model instantiated\n","- encoder.block.5.layer.1.DenseReluDense.wo.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([1850, 2048]) in the model instantiated\n","- encoder.block.5.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- encoder.final_layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- decoder.embed_tokens.weight: found shape torch.Size([32100, 512]) in the checkpoint and torch.Size([32100, 1850]) in the model instantiated\n","- decoder.block.0.layer.0.SelfAttention.q.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.0.layer.0.SelfAttention.k.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.0.layer.0.SelfAttention.v.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.0.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([1850, 512]) in the model instantiated\n","- decoder.block.0.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- decoder.block.0.layer.1.EncDecAttention.q.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.0.layer.1.EncDecAttention.k.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.0.layer.1.EncDecAttention.v.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.0.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([1850, 512]) in the model instantiated\n","- decoder.block.0.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- decoder.block.0.layer.2.DenseReluDense.wi.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([2048, 1850]) in the model instantiated\n","- decoder.block.0.layer.2.DenseReluDense.wo.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([1850, 2048]) in the model instantiated\n","- decoder.block.0.layer.2.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- decoder.block.1.layer.0.SelfAttention.q.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.1.layer.0.SelfAttention.k.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.1.layer.0.SelfAttention.v.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.1.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([1850, 512]) in the model instantiated\n","- decoder.block.1.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- decoder.block.1.layer.1.EncDecAttention.q.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.1.layer.1.EncDecAttention.k.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.1.layer.1.EncDecAttention.v.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.1.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([1850, 512]) in the model instantiated\n","- decoder.block.1.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- decoder.block.1.layer.2.DenseReluDense.wi.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([2048, 1850]) in the model instantiated\n","- decoder.block.1.layer.2.DenseReluDense.wo.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([1850, 2048]) in the model instantiated\n","- decoder.block.1.layer.2.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- decoder.block.2.layer.0.SelfAttention.q.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.2.layer.0.SelfAttention.k.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.2.layer.0.SelfAttention.v.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.2.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([1850, 512]) in the model instantiated\n","- decoder.block.2.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- decoder.block.2.layer.1.EncDecAttention.q.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.2.layer.1.EncDecAttention.k.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.2.layer.1.EncDecAttention.v.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.2.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([1850, 512]) in the model instantiated\n","- decoder.block.2.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- decoder.block.2.layer.2.DenseReluDense.wi.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([2048, 1850]) in the model instantiated\n","- decoder.block.2.layer.2.DenseReluDense.wo.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([1850, 2048]) in the model instantiated\n","- decoder.block.2.layer.2.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- decoder.block.3.layer.0.SelfAttention.q.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.3.layer.0.SelfAttention.k.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.3.layer.0.SelfAttention.v.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.3.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([1850, 512]) in the model instantiated\n","- decoder.block.3.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- decoder.block.3.layer.1.EncDecAttention.q.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.3.layer.1.EncDecAttention.k.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.3.layer.1.EncDecAttention.v.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.3.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([1850, 512]) in the model instantiated\n","- decoder.block.3.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- decoder.block.3.layer.2.DenseReluDense.wi.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([2048, 1850]) in the model instantiated\n","- decoder.block.3.layer.2.DenseReluDense.wo.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([1850, 2048]) in the model instantiated\n","- decoder.block.3.layer.2.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- decoder.block.4.layer.0.SelfAttention.q.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.4.layer.0.SelfAttention.k.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.4.layer.0.SelfAttention.v.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.4.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([1850, 512]) in the model instantiated\n","- decoder.block.4.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- decoder.block.4.layer.1.EncDecAttention.q.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.4.layer.1.EncDecAttention.k.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.4.layer.1.EncDecAttention.v.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.4.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([1850, 512]) in the model instantiated\n","- decoder.block.4.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- decoder.block.4.layer.2.DenseReluDense.wi.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([2048, 1850]) in the model instantiated\n","- decoder.block.4.layer.2.DenseReluDense.wo.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([1850, 2048]) in the model instantiated\n","- decoder.block.4.layer.2.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- decoder.block.5.layer.0.SelfAttention.q.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.5.layer.0.SelfAttention.k.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.5.layer.0.SelfAttention.v.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.5.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([1850, 512]) in the model instantiated\n","- decoder.block.5.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- decoder.block.5.layer.1.EncDecAttention.q.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.5.layer.1.EncDecAttention.k.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.5.layer.1.EncDecAttention.v.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([512, 1850]) in the model instantiated\n","- decoder.block.5.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 512]) in the checkpoint and torch.Size([1850, 512]) in the model instantiated\n","- decoder.block.5.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- decoder.block.5.layer.2.DenseReluDense.wi.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([2048, 1850]) in the model instantiated\n","- decoder.block.5.layer.2.DenseReluDense.wo.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([1850, 2048]) in the model instantiated\n","- decoder.block.5.layer.2.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- decoder.final_layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([1850]) in the model instantiated\n","- lm_head.weight: found shape torch.Size([32100, 512]) in the checkpoint and torch.Size([32100, 1850]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(32100, 1850)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32100, 1850)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1850, out_features=512, bias=False)\n","              (k): Linear(in_features=1850, out_features=512, bias=False)\n","              (v): Linear(in_features=1850, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=1850, bias=False)\n","              (relative_attention_bias): Embedding(32, 8)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=1850, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=1850, bias=False)\n","              (dropout): Dropout(p=0.5, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1850, out_features=512, bias=False)\n","              (k): Linear(in_features=1850, out_features=512, bias=False)\n","              (v): Linear(in_features=1850, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=1850, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=1850, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=1850, bias=False)\n","              (dropout): Dropout(p=0.5, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1850, out_features=512, bias=False)\n","              (k): Linear(in_features=1850, out_features=512, bias=False)\n","              (v): Linear(in_features=1850, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=1850, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=1850, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=1850, bias=False)\n","              (dropout): Dropout(p=0.5, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1850, out_features=512, bias=False)\n","              (k): Linear(in_features=1850, out_features=512, bias=False)\n","              (v): Linear(in_features=1850, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=1850, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=1850, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=1850, bias=False)\n","              (dropout): Dropout(p=0.5, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1850, out_features=512, bias=False)\n","              (k): Linear(in_features=1850, out_features=512, bias=False)\n","              (v): Linear(in_features=1850, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=1850, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=1850, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=1850, bias=False)\n","              (dropout): Dropout(p=0.5, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1850, out_features=512, bias=False)\n","              (k): Linear(in_features=1850, out_features=512, bias=False)\n","              (v): Linear(in_features=1850, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=1850, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=1850, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=1850, bias=False)\n","              (dropout): Dropout(p=0.5, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32100, 1850)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1850, out_features=512, bias=False)\n","              (k): Linear(in_features=1850, out_features=512, bias=False)\n","              (v): Linear(in_features=1850, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=1850, bias=False)\n","              (relative_attention_bias): Embedding(32, 8)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1850, out_features=512, bias=False)\n","              (k): Linear(in_features=1850, out_features=512, bias=False)\n","              (v): Linear(in_features=1850, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=1850, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=1850, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=1850, bias=False)\n","              (dropout): Dropout(p=0.5, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1850, out_features=512, bias=False)\n","              (k): Linear(in_features=1850, out_features=512, bias=False)\n","              (v): Linear(in_features=1850, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=1850, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1850, out_features=512, bias=False)\n","              (k): Linear(in_features=1850, out_features=512, bias=False)\n","              (v): Linear(in_features=1850, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=1850, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=1850, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=1850, bias=False)\n","              (dropout): Dropout(p=0.5, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1850, out_features=512, bias=False)\n","              (k): Linear(in_features=1850, out_features=512, bias=False)\n","              (v): Linear(in_features=1850, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=1850, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1850, out_features=512, bias=False)\n","              (k): Linear(in_features=1850, out_features=512, bias=False)\n","              (v): Linear(in_features=1850, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=1850, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=1850, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=1850, bias=False)\n","              (dropout): Dropout(p=0.5, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1850, out_features=512, bias=False)\n","              (k): Linear(in_features=1850, out_features=512, bias=False)\n","              (v): Linear(in_features=1850, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=1850, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1850, out_features=512, bias=False)\n","              (k): Linear(in_features=1850, out_features=512, bias=False)\n","              (v): Linear(in_features=1850, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=1850, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=1850, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=1850, bias=False)\n","              (dropout): Dropout(p=0.5, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1850, out_features=512, bias=False)\n","              (k): Linear(in_features=1850, out_features=512, bias=False)\n","              (v): Linear(in_features=1850, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=1850, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1850, out_features=512, bias=False)\n","              (k): Linear(in_features=1850, out_features=512, bias=False)\n","              (v): Linear(in_features=1850, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=1850, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=1850, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=1850, bias=False)\n","              (dropout): Dropout(p=0.5, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1850, out_features=512, bias=False)\n","              (k): Linear(in_features=1850, out_features=512, bias=False)\n","              (v): Linear(in_features=1850, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=1850, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1850, out_features=512, bias=False)\n","              (k): Linear(in_features=1850, out_features=512, bias=False)\n","              (v): Linear(in_features=1850, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=1850, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=1850, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=1850, bias=False)\n","              (dropout): Dropout(p=0.5, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.5, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=1850, out_features=32100, bias=False)\n",")"]},"metadata":{},"execution_count":70}]},{"cell_type":"code","source":["dataset_train = TensorDataset(input_ids_train, \n","                              attention_masks_train,\n","                              labels_train)\n","\n","dataset_val = TensorDataset(input_ids_val, \n","                             attention_masks_val, \n","                             labels_val)\n","\n","dataset_test = TensorDataset(input_ids_test, \n","                             attention_masks_test, \n","                             labels_test,\n","                             expected_source_code)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":371},"id":"QbyPuDvYIjQq","executionInfo":{"status":"error","timestamp":1651904948966,"user_tz":240,"elapsed":209,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}},"outputId":"8488cd61-a685-40ee-f31e-37ddd8ad4d9f"},"execution_count":97,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-97-01933f2ad229>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                              \u001b[0mattention_masks_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                              \u001b[0mlabels_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                              expected_source_code)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Size mismatch between tensors\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Size mismatch between tensors\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"]}]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","batch_size = 4\n","\n","#train set\n","dataloader_train = DataLoader(dataset_train,\n","                              sampler = RandomSampler(dataset_train),\n","                              batch_size = batch_size)\n","\n","#validation set\n","dataloader_val = DataLoader(dataset_val,\n","                              sampler = RandomSampler(dataset_val),\n","                              batch_size = batch_size)\n","\n","dataloader_test = DataLoader(dataset_val,\n","                              sampler = RandomSampler(dataset_val),\n","                              batch_size = batch_size)"],"metadata":{"id":"3KC16GLEImTA","executionInfo":{"status":"ok","timestamp":1651904207886,"user_tz":240,"elapsed":207,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["from transformers import get_linear_schedule_with_warmup\n","\n","optimizer = torch.optim.AdamW(t5_model.parameters(),\n","                 lr = 1e-5,\n","                 eps = 1e-8)\n","                 \n","epochs = 5\n","\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                           num_warmup_steps = 0,\n","                                           num_training_steps = len(dataloader_train)*epochs)"],"metadata":{"id":"9y6eZGrsIql-","executionInfo":{"status":"ok","timestamp":1651904209624,"user_tz":240,"elapsed":318,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}}},"execution_count":84,"outputs":[]},{"cell_type":"code","source":["def evaluate(dataloader_val,model):\n","\n","    #evaluation mode \n","    model.eval()\n","    \n","    #tracking variables\n","    loss_val_total = 0\n","    \n","    for batch in tqdm(dataloader_val):\n","        \n","        #load into GPU\n","        batch = tuple(b.to(device) for b in batch)\n","        \n","        #define inputs\n","        inputs = {'input_ids':      batch[0],\n","                  'attention_mask': batch[1],\n","                  'labels':         batch[2]}\n","\n","        #compute logits\n","        with torch.no_grad():        \n","            outputs = model(**inputs)\n","        \n","        #compute loss\n","        loss = outputs[0]\n","        loss_val_total += loss.item()\n","\n","    #compute average loss\n","    loss_val_avg = loss_val_total/len(dataloader_val) \n","    \n","            \n","    return loss_val_avg"],"metadata":{"id":"BFwFGwPuI0TF","executionInfo":{"status":"ok","timestamp":1651904555659,"user_tz":240,"elapsed":207,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}}},"execution_count":91,"outputs":[]},{"cell_type":"code","source":["def train_model(model):\n","    save_epoch=5\n","    validation_loss_epochs=[]\n","    training_loss_epochs=[]\n","\n","    for epoch in tqdm(range(1, epochs+1)):\n","        \n","        model.train()\n","        \n","        loss_train_total = 0\n","        \n","        progress_bar = tqdm(dataloader_train, \n","                            desc = 'Epoch {:1d}'.format(epoch), \n","                            leave = False, \n","                            disable = False)\n","        \n","        for batch in progress_bar:\n","            \n","            model.zero_grad() #set gradient to 0\n","        \n","            batch = tuple(b.to(device) for b in batch)\n","            \n","            inputs = {'input_ids': batch[0], \n","                      'attention_mask': batch[1], \n","                      'labels': batch[2]}\n","            \n","            outputs = model(**inputs) #unpack the dict straight into inputs\n","            \n","            loss = outputs[0]\n","            loss_train_total += loss.item()\n","            loss.backward()\n","            \n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            \n","            optimizer.step()\n","            scheduler.step()\n","            progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item() / len(batch))})\n","\n","        if epoch%save_epoch==0:    \n","          torch.save(model.state_dict(), f'Models/ CodeT5_ft_epoch{epoch}.model')\n","        \n","        tqdm.write('\\n Epoch {epoch}')\n","        \n","        loss_train_ave = loss_train_total / len(dataloader_train)\n","        tqdm.write('Training loss: {loss_train_avg}')\n","        \n","        val_loss = evaluate(dataloader_val,model)\n","        validation_loss_epochs.append(val_loss)\n","        training_loss_epochs.append(loss_train_ave)\n","        \n","        tqdm.write(f'Validation loss: {val_loss}')\n","        return validation_loss_epochs,training_loss_epochs,model"],"metadata":{"id":"Ewqvg0THI8f1","executionInfo":{"status":"ok","timestamp":1651904560278,"user_tz":240,"elapsed":212,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}}},"execution_count":92,"outputs":[]},{"cell_type":"code","source":["val_losses,training_losses,t5_model=train_model(t5_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":478},"id":"8PTEB0FlKDaY","executionInfo":{"status":"error","timestamp":1651904216324,"user_tz":240,"elapsed":299,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}},"outputId":"63728958-ca68-4edf-a873-aafde77c93f1"},"execution_count":87,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/5 [00:00<?, ?it/s]\n","Epoch 1:   0%|          | 0/4590 [00:00<?, ?it/s]\u001b[A\n","  0%|          | 0/5 [00:00<?, ?it/s]\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-87-7fed75de5d16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt5_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt5_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-86-4263b86fb5fc>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     25\u001b[0m                       'labels': batch[2]}\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#unpack the dict straight into inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1606\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1609\u001b[0m             )\n\u001b[1;32m   1610\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseModelOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 )\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         )\n\u001b[1;32m    677\u001b[0m         \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpresent_key_value_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         )\n\u001b[1;32m    583\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;31m# compute scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         scores = torch.matmul(\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m         )  # equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 124.00 MiB (GPU 0; 14.76 GiB total capacity; 12.63 GiB already allocated; 7.75 MiB free; 13.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]},{"cell_type":"code","source":["import torch, gc\n","gc.collect()\n","torch.cuda.empty_cache()"],"metadata":{"id":"gX_40sFWKIm8","executionInfo":{"status":"ok","timestamp":1651904571017,"user_tz":240,"elapsed":192,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}}},"execution_count":93,"outputs":[]},{"cell_type":"code","source":["def getPredictions(inputs,model):\n","      sample_outputs = model.generate(\n","        input_ids.to(device),\n","        do_sample=True, \n","        max_length=50, \n","        top_k=50, \n","        top_p=0.95, \n","        num_return_sequences=3)\n","    # outputs = model.generate(input_ids)\n","      return tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n","\n","  def getPredictionForDataLoader()"],"metadata":{"id":"4t-MOhnsKq-o","executionInfo":{"status":"ok","timestamp":1651904387539,"user_tz":240,"elapsed":186,"user":{"displayName":"Harish Ravi","userId":"04360313370642922621"}}},"execution_count":88,"outputs":[]}]}